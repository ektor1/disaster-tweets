{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_sample = pd.read_csv(\"data/sample_submission.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data (7613, 5)\n",
      "Index(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n",
      "Test data (3263, 4)\n",
      "Index(['id', 'keyword', 'location', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\", df_train.shape)\n",
    "print(df_train.columns)\n",
    "print(\"Test data\", df_test.shape)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_func import preprocess\n",
    "X_train = [preprocess(tweet) for tweet in df_train[\"text\"]]\n",
    "X_test = [preprocess(tweet) for tweet in df_test[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected - Output Label:  1\n",
      "['residents', 'asked', 'place', 'notified', 'officers', 'evacuation', 'shelter', 'place', 'orders', 'expected']\n"
     ]
    }
   ],
   "source": [
    "# Compare raw with tokenised tweet\n",
    "print(df_train[\"text\"][2], \"- Output Label: \", df_train[\"target\"][2])\n",
    "print(X_train[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make new columns with tokenised sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['token_sentence'] = pd.NaT # Create new column to store tokenized sentences\n",
    "tok_column = df_train.pop('token_sentence')\n",
    "df_train.insert(0,'token_sentence', tok_column) # Shift column to first position in df\n",
    "df_train['token_sentence'] = X_train\n",
    "\n",
    "df_test['token_sentence'] = pd.NaT # Create new column to store tokenized sentences\n",
    "tok_column = df_test.pop('token_sentence')\n",
    "df_test.insert(0,'token_sentence', tok_column) # Shift column to first position in df\n",
    "df_test['token_sentence'] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['token_sentence']\n",
    "y_train = df_train['target']\n",
    "X_test = df_test['token_sentence']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google News\n",
    "Model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/Users/ektornikolinakos/working/vscode/comply/models/GoogleNews-vectors-negative300.bin.gz', binary=True,)\n",
    "# FastText\n",
    "# Model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    # 'datasets/GoogleNews-vectors-negative300.bin.gz', binary=True,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further tweet cleaning\n",
    "There are a handful of sentences that have only one tokenized word and have a positive label. I don't see how these would help the model to identify disaster tweets, but we if had a larger dataset I could imagine a few cases where tweets including words help, disaster, amargedon would be labeled as positives. Because these sentences do not provide useful information I have decided to remove them, but if we had a larger dataset I would just remove the empty tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crash']\n",
      "['jorrynja']\n",
      "[]\n",
      "['prob']\n",
      "['http']\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(df_train['token_sentence']):\n",
    "    if len(j) < 2 and df_train['target'][i] == 1:\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 rows removed\n"
     ]
    }
   ],
   "source": [
    "from tweet_func import filter_docs # remove tweets with 1 or 0 words\n",
    "tweets = [X_train]\n",
    "labels = [y_train]\n",
    "filter_docs(tweets, labels, lambda text: (len(text)<2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 rows removed\n"
     ]
    }
   ],
   "source": [
    "from tweet_func import filter_docs, has_vector_representation # Removes words not found in the Google News dictionary\n",
    "filter_docs(tweets, labels, lambda text: has_vector_representation(Model, text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word experimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from tweet_func import stemming\n",
    "stem_train = [stemming(sentence) for sentence in X_train]\n",
    "stem_test = [stemming(sentence) for sentence in X_test]\n",
    "stem_valid = [stemming(sentence) for sentence in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from tweet_func import average_vecs\n",
    "trainVecs = [average_vecs(sentence, Model, 300) for sentence in stem_train]\n",
    "testVecs = [average_vecs(sentence, Model, 300) for sentence in stem_test]\n",
    "validVecs = [average_vecs(sentence, Model, 300) for sentence in stem_valid]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average vectors for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average vectors for each sentence\n",
    "from tweet_func import average_vecs\n",
    "trainVecs = [average_vecs(sentence, Model, 300) for sentence in X_train]\n",
    "testVecs = [average_vecs(sentence, Model, 300) for sentence in X_test]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree to check how submission works\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(trainVecs, y_train)\n",
    "y_pred = clf.predict(testVecs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_score = 0.68617"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest with random grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# distributions is a dictionary of different parameters that the random forest can take\n",
    "# Random grid search will pick 100 random combinations of those parameters\n",
    "# We will evaluate the best model by using a 3-fold cross validation and the f1 score\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "distributions = dict(n_estimators=range(100,2000,200),\n",
    "                    max_depth=range(5,100,5))\n",
    "clf = RandomizedSearchCV(random_forest, distributions, n_iter=10, cv=3, scoring=\"f1\", random_state=42)\n",
    "clf.fit(trainVecs, y_train)\n",
    "\n",
    "y_pred = clf.predict(testVecs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1_score = 0.79037"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning - Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample[\"target\"] = y_pred\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
